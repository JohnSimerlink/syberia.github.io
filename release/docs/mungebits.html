<html>

  <head>
    <script src="https://use.typekit.net/xdj4hmw.js"></script>
    <script type="text/javascript">try { Typekit.load({ async: true }); } catch(e) { }</script>

    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,500" rel="stylesheet" type="text/css">
    <link rel="stylesheet" media="all" href="../style.css" type="text/css" />
    <link rel="stylesheet" media="all" href="../stylesheets/rocco.css" />
    <link rel="stylesheet" media="all" href="../stylesheets/github-markdown.css" />

    <script src="../assets/highlight.pack.js"></script>
    <script type="text/javascript">
      hljs.initHighlightingOnLoad();
    </script>
  </head>

  <body>
  <div class="backdrop tk-proxima-nova">
    <div class="header">
      <div class="header-left">Syberia</div>
      <div class="header-right">
        <ul>
          <li><a href="#">Get Started</a></li>
          <li><a href="#">Docs</a></li>
        </ul>
      </div>
    </div>
    <br /> <br /> <br />
  </div>
  <div class="colmask leftmenu tk-proxima-nova">
    <div class="colleft">
      <div class="col1">
<h1>Mungebits</h1>

<p>Machine learning looks at first glance like the science of 
devising and answering pure statistical questions on real-life data sets.
In practice, a lot of it ends up being
<a target="_new" href="http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0">data janitor work</a>.
</p>

<p>Turning the data cleaning and feature engineering process into
a quick workflow and as pleasant a task as trying out different modeling methodologies 
will eventually address this problem.</p>

<p> Let's restrict
the discussion to transforming one messy, raw dataset into a 
clean, ready-to-model dataset. Parsing and merging of data is not
typically R's forte so we will solve the single dataset problem
before proceeding with more complicated cleaning topologies.</p>

<p>The initial abstraction offered by the Syberia modeling engine is that
of <b>mungebits</b>: atomic production-ready feature engineering templates
that can be used to quickly clean a dataset without having to write
separate code when it comes time to replicate the cleaning process
on single rows of data.</p>

<p>Why would we want to replicate the entire process? That's precisely
one of the difficulties that emerges when it comes time to "productionize"
the model! The trained classifier expects a certain type of covariate matrix,
namely the one in exactly the preprocessed format you generated by cleaning
the data set.</p>

<p>But why would we want to think about "productionizing" right away
when we are experimenting? Because lack of due consideration there
is the primary obstacle leading to weeks or months long error-prone
translation processes when turning an experimental result into one
that can be applied on new data in the real world.</p>

<p>It's worth investing a little bit more time now instead of far more time later
when we have to replicate the cleaning work on new data. Note this 
does not just apply to streaming systems! If someone gave you a new
validation set from their data warehouse and asked to verify the
performance on some new data points it would be equivalent to
solving the streaming problem: we would have to replicate the entire
cleaning process on the raw validation set.
</p>

<p>For example, if you replaced messed up values with the correct
values, imputed missing values, dropped certain columns, or derived some
new features from the existing data, you will have to re-do that work
when a single data point comes in--otherwise the trained classifier
will be confused.</p>

<div class="img-right">
  <img src="../../images/narrow_vs_wide.png" title="Narrow and Wide Transformations" />
</div>

<p>Operations that take one <code class="inline">data.frame</code>
and return another <code class="inline">data.frame</code> come
in two types: <i>narrow</i> and <i>wide</i> transformations. You can
delegate the <i>narrow</i> transformations to an ETL engineer or other
data curation role. However, this might not be feasible if we are deploying
a model, because they will have to provide a way to replicate their 
process in real-time as well, for example through an API.

<p>Narrow transformations, operations that can be done row-by-row without
looking at the other rows, almost by definition do not need to store metadata to
be reproduced on a single row. However, many of the most useful cleaning
operations on a dataset&mdash;like sure independence screening, principal component
analysis, imputation, discretization, aggregation and computation of ratios,
numericalization of categorical features, and so on&mdash;are
<i>wide</i> transformations.</p>

<p>The abundance of useful wide transformations
is the reason why <b>feature engineering cannot be separated from
statistical modeling</b>. It is impossible to have one person's role
be maintaining a data pipeline while another person independently maintains
a statistical model without running into a
<a target="_new" href="http://research.google.com/pubs/pub43146.html">messy combinatorial explosion</a>:
they are inseparable aspects of the same task,
in the same way database operations cannot be separated from a web
application but instead compose its heart.</p>

<p>Let's convince ourselves that this is true.</p>

<h2>The Basic Mungebit</h2>

<p>Every time you write a script that begins with <code class="inline">read.csv("data.csv")</code>
and ends with a clean data set you have actually produced an
<i>endomorphism on the space of dataframes</i>, that is, a function
which takes one dataframe and yields another dataframe. Functions that
have the same domain and co-domain are really cool because they can
be <i>trivially composed</i>: if I have several such functions, I can just
apply them one after the other without worrying about setting up a complex
input/output graph since I know the input and the result will always be
a dataframe. (For the mathematically inclined, endomorphisms of any space form
a monoid under composition.)</p>

<p>In algebra class we were able to prove beautiful facts about parabolas and
the other conic sections because we looked at the most general cases, like
<code class="inline">y = a * x^2 + b * x + c</code>. Here, <code class="inline">a</code>,
<code class="inline">b</code>, and <code class="inline">c</code> are parameters:
for any combination of such three constants drawn from the real numbers we
get a different parabola.</p>

<p>Parametrization is powerful because it means we don't have to repeat our
work every time a special case comes along. Finding the quadratic equation can be
done once; the other times we simply plug in the parameters! Every time you write
the type of code below you are re-deriving the quadratic formula by hand.</p>

<div class="code">
  <pre>
    <code class="R"><span class="spacer">
# Uncomment the line below if you want to run this on an example.
# raw_data <- data.frame(occupation = c("Doctor", rep("Waiter", 200)), stringsAsFactors = FALSE)
raw_data$occupation <- ave(raw_data$occupation, raw_data$occupation, FUN = function(x) {
  if (length(x) / NROW(raw_data) < 0.01) { # Percentage of population with this occupation.
    "Other"
  } else {
    x[1L]
  }
})
    </span></code>
  </pre>
</div>

<p>If we later decide to perform the same operation on another variable, the path
of lowest resistance will be copying the above and replacing "occupation" with
the other variable name. If instead we had parametrized this as a function
that took a dataset, the variables to alter, and a minimal threshold for replacing
the value with "Other," we could easily re-use it for a variety of datasets.</p>

<p>What about when it comes time to replay the operation on a single row of data
or a new validation set? <b>The above code won't run!</b> With a single row of
data, all values will always have trivially 100% incidence and nothing will
ever be replaced with "Other," which might lead to unexpected results
when it comes to our classifier. Now imagine this problem amplified by
100 more feature engineering steps on a dataset with 1000s of variables.
This is why data cleaning is seen as a combinatorial nightmare.</p>

<p>But it does not have to be. Mungebits solve this problem, and a proper
generalization of mungebits solves any problem of taking raw, messy
data sets, joining and munging them until there's a clean covariate matrix
on which we can train a model&mdash;without having to ever write a custom data pipeline,
instead making the process part of the explorative data science journey.
</p>

<p>So what does a mungebit for the above step look like?</p>

<div class="code">
  <pre>
    <code class="R"><span class="spacer">
# lib/mungebits/remove_uncommon_values.R
train <- function(dataframe, variables, threshold = 0.01, replacement = "Other") {
  replacements <- list()
  for (v in variables) {
    dataframe[[v]] <- ave(dataframe[[v]], dataframe[[v]], FUN = function(x) {
      if (length(x) / NROW(dataframe) < threshold) {
        replacements[[v]] <<- c(replacements[[v]], x[1L])
        "Other"
      } else {
        x[1L]
      }
    })
  }
  input$replacements <- replacements
  dataframe
}

predict <- function(dataframe, variables, threshold = 0.01, replacement = "Other") {
  variables <- intersect(colnames(dataframe), variables)
  for (v in names(input$replacements)) {
    dataframe[[v]][dataframe[[v]] %in% input$replacements[[v]]] <- replacement
  }
  dataframe
}
    </span></code>
  </pre>
</div>

<p>So we had to do a little more work. The advantage is that we will never
have to think about the problem of replacing columns with rare values again:
we will be able to pass different parameters to the mungebit, and when it is
trained we will be able to re-use it on arbitrary future data sets, including
single rows for new data points. We can grab our mungebit within the
Syberia session using <code class="inline">resource("lib/mungebits/remove_uncommon_values")</code>.</p>

<p>
  <img src="../../images/mungebit_example.png" title="Mungebit example" />
</p>

<p>Let's see what happens when we run it on a simple example.</p>

<div class="code">
  <pre>
    <code class="R"><span class="spacer">
bit <- resource("lib/mungebits/remove_uncommon_values")
iris$Species <- as.character(iris$Species)
iris[1, "Species"] <- "Bumblebee"
head(bit$run(iris, "Species"))
#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 1          5.1         3.5          1.4         0.2   Other
# 2          4.9           3          1.4         0.2  setosa
    </span></code>
  </pre>
</div>

<p>What happens when we run it on a single row of iris data?</p>

<div class="code">
  <pre>
    <code class="R"><span class="spacer">
bit$run(iris[1, ], "Species") # We never modified iris.
#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 1          5.1         3.5          1.4         0.2   Other
    </span></code>
  </pre>
</div>

<p>Our feature engineering step is now "production ready"! We can
replay it on streaming rows of new data or on new validation sets
to determine whether our classifier is behaving as expected.</p>

<div class="code">
  <pre>
    <code class="R"><span class="spacer">
bit$train(iris[1, ])
# Error:
#   This mungebit has already been trained, cannot re-train.
    </span></code>
  </pre>
</div>

<p>Training a mungebit is like flipping a one-time switch: once
we do it, we can't undo it; we would need a new fresh mungebit.
This way, we can be sure that once we have fed in our initial training
set, its specific characteristics will be used to replay the 
feature engineering example as it happened.</p>

<p>For example, if a rare
value was on the border of the 0.01 threshold, certain subsamples
of the training set might consider it "rare" and replaceable with
"Other" whilst others do not. If we didn't have mungebits, we would
need to write a separate data pipeline for each model as to be
very careful about these sorts of edge cases. Incorrectly failing 
to replace a value could lead to bizarre bugs in our classifier
when the column gets converted to a categorical feature. Instead, the unified
<code class="inline">bit$run(...)</code> interface means the only
time we will ever have to think about the train versus predict
distinction is when we are writing our mungebit.</p>

<p>Let's write some tests to convince ourselves our implementation is
valid.</p>

<div class="code">
  <pre>
    <code class="R"><span class="spacer">
test_that("it does not replace a common value", {
  bit <- resource()
  iris$Species <- as.character(iris$Species)
  # Test the mungebit when it is training.
  expect_equal(as.character(bit$run(iris, "Species")[1, 5]), "setosa")
  stopifnot(bit$trained())
  # And when it is predicting.
  expect_equal(as.character(bit$run(iris, "Species")[1, 5]), "setosa")
})
    </span></code>
  </pre>
</div>

<p style="text-align: center">
  <img src="../../images/mungebit_test_example.png" title="Mungebit Test Example" />
</p>

<p>Looking pretty good! Note we used <code class="inline">stest</code>, a helper
provided by the Syberia modeling engine, to test our "resource", in this case the mungebit.</p>

<p>The point of good tests is to anticipate not only the example for which you wrote
the code, but every possible input anyone could ever throw at it, so there are no
surprises if someone uses your code in a way you did not expect.</p>

<div class="code">
  <pre>
    <code class="R"><span class="spacer">
test_that("it replaces an uncommon value", {
  bit <- resource()
  iris$Species <- as.character(iris$Species)
  iris[1, 5] <- "Bumblebee"
  expect_equal(bit$run(iris, "Species")[1, 5], "Other")
  expect_equal(bit$run(iris, "Species")[1, 5], "Other")
})

test_that("it replaces a previously-unseen value", {
  bit <- resource()
  iris$Species <- as.character(iris$Species)
  iris[1, 5] <- "Bumblebee"
  expect_equal(bit$run(iris, "Species")[1, 5], "Other")
  iris[1, 5] <- "Flubber"
  expect_equal(bit$run(iris, "Species")[1, 5], "Other")
})
    </span></code>
  </pre>
</div>

<p style="text-align: center">
  <img src="../../images/mungebit_test_failure.png" title="Mungebit Test Failure" />
</p>

<p>Oh no! Our first <i>test failure</i>. Writing tests is about stepping outside
of the box of how you think about the work you have written and considering every
possible scenario for how other people may use it. In this case, if we had encountered
a new data point with a value that was not observed during the training process,
we would have failed to replace it with "Other" &mdash; even though its incidence
in the original data was 0%!</p>

<p>This is a very common oversight. While writing the original code, we assumed
that we wish to replace values that are uncommon. We stored them for later
so that we could repeat the same process when new data came in. However, we failed
to take into account that future data could have an <i>infinite number of possible
  values</i>. We should have instead stored the very finite list (by the pigeonhole principle,
at most 100) of <i>common</i> values: those with incidence more than 1%.</p>

<div class="code">
  <pre>
    <code class="R"><span class="spacer">
# lib/mungebits/remove_uncommon_values.R
train <- function(dataframe, variables, threshold = 0.01, replacement = "Other") {
  common_values <- list()
  for (v in variables) {
    dataframe[[v]] <- ave(dataframe[[v]], dataframe[[v]], FUN = function(x) {
      if (length(x) / NROW(dataframe) < threshold) {
        "Other"
      } else {
        common_values[[v]] <<- c(common_values[[v]], x[1L])
        x[1L]
      }
    })
  }
  input$common_values <- common_values
  dataframe
}

predict <- function(dataframe, variables, threshold = 0.01, replacement = "Other") {
  variables <- intersect(colnames(dataframe), variables)
  for (v in names(input$common_values)) {
    dataframe[[v]][!is.element(dataframe[[v]], input$common_values[[v]])] <- replacement
  }
  dataframe
}
    </span></code>
  </pre>
</div>

<p style="text-align: center">
  <img src="../../images/mungebit_test_fixed.png" title="Mungebit Test Fixed" />
</p>

<p>The more tests we write, the more <i>coverage</i> we have. You can think of
test coverage (how much of your code is covered by test cases) as insurance
on future changes to the code base. As the amount of code we write increases
and the project becomes more complex, it will be harder and harder to track
what everyone's assumptions are about the inputs and outputs to all the functions
and resources used throughout the project. If someone ever fixes a bug or
refactors something, having lots of tests means it will be very easy to tell
if they broke something in the process.</p>



        </div>
        <div class="col2">
          <br /> <br /> <br />
          SIDEBAR TEXT!
        </div>
      </div>
    </div>
  </div>
</body>
